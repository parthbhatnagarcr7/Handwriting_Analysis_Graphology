{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63396f37-8588-4b7d-a84d-362d4ef0ae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\parth\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Canvas cleared\n",
      "Canvas cleared\n",
      "Canvas cleared\n",
      "Canvas cleared\n",
      "Canvas cleared\n",
      "Attempting to save canvas\n",
      "Canvas saved successfully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "# Function to get the index finger tip and thumb tip positions\n",
    "def get_finger_tips(frame, hand_landmarks):\n",
    "    if hand_landmarks:\n",
    "        index_finger_tip = hand_landmarks.landmark[mp.solutions.hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "        thumb_tip = hand_landmarks.landmark[mp.solutions.hands.HandLandmark.THUMB_TIP]\n",
    "        height, width, _ = frame.shape\n",
    "        index_finger_tip_px = (int(index_finger_tip.x * width), int(index_finger_tip.y * height))\n",
    "        thumb_tip_px = (int(thumb_tip.x * width), int(thumb_tip.y * height))\n",
    "        return index_finger_tip_px, thumb_tip_px\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Function to calculate distance between two points\n",
    "def calculate_distance(point1, point2):\n",
    "    return math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "# Function to perform smoothing\n",
    "def smooth_line(new_point, prev_points, smoothing_factor=0.5):\n",
    "    if prev_points is None:\n",
    "        return new_point\n",
    "    else:\n",
    "        smoothed_point = tuple(np.round(smoothing_factor * np.array(new_point) + (1 - smoothing_factor) * np.array(prev_points)).astype(int))\n",
    "        return smoothed_point\n",
    "\n",
    "def main():\n",
    "    # Open camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Initialize MediaPipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(max_num_hands=1)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    # Create a blank canvas filled with white color\n",
    "    canvas = np.ones((480, 640, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Variables for drawing and erasing\n",
    "    drawing = False\n",
    "    prev_point = None\n",
    "\n",
    "    # Create resizable windows\n",
    "    cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)\n",
    "    cv2.namedWindow('Canvas', cv2.WINDOW_NORMAL)\n",
    "    # Set initial window sizes\n",
    "    cv2.resizeWindow('Frame', 1020, 720)\n",
    "    cv2.resizeWindow('Canvas', 1020, 720)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Adjust brightness\n",
    "        frame = cv2.convertScaleAbs(frame, alpha=0.7, beta=30)  # adjust alpha and beta as needed\n",
    "\n",
    "        # Convert the BGR image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        # Draw hand landmarks on the frame\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Get finger tip positions\n",
    "        index_finger_tip, thumb_tip = get_finger_tips(frame, results.multi_hand_landmarks[0] if results.multi_hand_landmarks else None)\n",
    "\n",
    "        if index_finger_tip is not None and thumb_tip is not None:\n",
    "            distance_threshold = 50  # Adjust threshold as needed\n",
    "            distance = calculate_distance(index_finger_tip, thumb_tip)\n",
    "            \n",
    "            if distance < distance_threshold:\n",
    "                drawing = False\n",
    "                # Check if index finger is touching the top part of the frame\n",
    "                if index_finger_tip[1] < 50:  # Adjust the value as needed\n",
    "                    # Implement clear, save, quit functionality\n",
    "                    if index_finger_tip[0] < 213:  # 640/3, 3 sections\n",
    "                        canvas = np.ones((480, 640, 3), dtype=np.uint8) * 255  # Clear canvas\n",
    "                        print(\"Canvas cleared\")\n",
    "                    elif 213 <= index_finger_tip[0] < 426:  # 640/3 * 2\n",
    "                        print(\"Attempting to save canvas\")\n",
    "                        saved = cv2.imwrite(r\"C:\\Users\\parth\\OneDrive\\Desktop\\IBM_Work\\Canvas.png\", canvas)  # Save canvas as image\n",
    "                        if saved:\n",
    "                            print(\"Canvas saved successfully\")\n",
    "                        else:\n",
    "                            print(\"Failed to save canvas\")\n",
    "                    elif 426 <= index_finger_tip[0] < 640:\n",
    "                        print(\"Quitting program\")\n",
    "                        break  # Quit program\n",
    "            else:\n",
    "                drawing = True\n",
    "\n",
    "        if drawing and index_finger_tip is not None:\n",
    "            cv2.circle(frame, index_finger_tip, 5, (0, 0, 0), -1)  # Draw in black color\n",
    "            if prev_point is not None:\n",
    "                smoothed_point = smooth_line(index_finger_tip, prev_point)\n",
    "                cv2.line(canvas, prev_point, smoothed_point, (0, 0, 0), 4)  # Draw in black color\n",
    "                prev_point = smoothed_point\n",
    "            else:\n",
    "                prev_point = index_finger_tip\n",
    "        else:\n",
    "            prev_point = None\n",
    "        \n",
    "        # Display canvas\n",
    "        cv2.imshow('Canvas', canvas)\n",
    "\n",
    "        # Overlay drawing onto the frame\n",
    "        frame_with_drawing = cv2.addWeighted(frame, 0.5, canvas, 0.5, 0)\n",
    "        \n",
    "        # Add functionality text\n",
    "        cv2.putText(frame_with_drawing, \"Clear\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "        cv2.putText(frame_with_drawing, \"Save\", (220, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(frame_with_drawing, \"Quit\", (420, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Display frame\n",
    "        cv2.imshow('Frame', frame_with_drawing)\n",
    "\n",
    "        # Check for key press\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01337987-374f-4c28-973f-4e4825a75a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '2 2 Vesoma.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "image_path = image_path = r\"C:\\Users\\parth\\OneDrive\\Desktop\\IBM_Work\\Graphology\\Handwriting_Analysis\\images\\images\\Canvas.png\"\n",
    "# Create the image-to-text pipeline using the TrOCR model\n",
    "pipe = pipeline(\"image-to-text\", model=\"microsoft/trocr-base-handwritten\")\n",
    "prediction = pipe(image_path)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a972c946-2530-4122-9192-3db8104261a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<s_cord-v2><s_menu><s_nm> Amari</s_nm><s_price>'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = r\"C:\\Users\\parth\\OneDrive\\Desktop\\IBM_Work\\Graphology\\Handwriting_Analysis\\images\\images\\Canvas.png\"\n",
    "pipe = pipeline(\"image-to-text\", model=\"jinhybr/OCR-Donut-CORD\")\n",
    "prediction = pipe(image_path)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f569a91-e135-4776-91e5-8130f0bd2e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "img_path = cv2.imread(r\"C:\\Users\\parth\\OneDrive\\Desktop\\IBM_Work\\Canvas.png\")\n",
    "img = cv2.cvtColor(img_path, cv2.COLOR_BGR2GRAY)\n",
    "pytesseract.image_to_string(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37209497-1b9c-4bf2-93e3-4ded519c5e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text block orientation: Horizontal\n",
      "Slant angle of the entire text region: -0.22 degrees\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image  # For potential deskewing (slant correction)\n",
    "def get_slant(gray):\n",
    "  # Calculate moments\n",
    "  moments = cv2.moments(gray)\n",
    "  mu02 = moments['mu02']\n",
    "  mu11 = moments['mu11']\n",
    "  # Check for division by zero\n",
    "  if abs(mu02) < 1e-6:\n",
    "    return 0\n",
    "  else:\n",
    "    # Calculate slant angle (radians)\n",
    "    return np.degrees(np.arctan((2 * mu11) / mu02))\n",
    "\n",
    "\n",
    "def get_text_block_orientation(gray):\n",
    "  # Apply thresholding and find contours\n",
    "  thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "  cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "  \n",
    "  # Check for detected contours\n",
    "  if len(cnts) == 0:\n",
    "    return \"Text not detected\"\n",
    "  \n",
    "  # Get bounding rectangle of all contours\n",
    "  x, y, w, h = cv2.boundingRect(np.vstack(cnts))\n",
    "  \n",
    "  # Calculate aspect ratio\n",
    "  aspect_ratio = w / float(h)\n",
    "  \n",
    "  # Estimate orientation based on aspect ratio (heuristic)\n",
    "  if aspect_ratio > 1:\n",
    "    orientation = \"Horizontal\"\n",
    "  else:\n",
    "    orientation = \"Vertical\"\n",
    "  \n",
    "  # Calculate slant angle\n",
    "  slant_angle = get_slant(gray[y:y+h, x:x+w])  # Apply slant calculation on the text region\n",
    "\n",
    "  return orientation, slant_angle\n",
    "\n",
    "img_path = r\"C:\\Users\\parth\\OneDrive\\Desktop\\IBM_Work\\Canvas.png\"\n",
    "img = cv2.imread(img_path)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Analyze text region\n",
    "orientation, slant_angle = get_text_block_orientation(gray)\n",
    "\n",
    "print(f\"Text block orientation: {orientation}\")\n",
    "print(f\"Slant angle of the entire text region: {slant_angle:.2f} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f8d05-e8c2-4b93-a6cd-6330b23652a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
